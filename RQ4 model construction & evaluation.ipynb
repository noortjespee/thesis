{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>education</th>\n",
       "      <th>distance_store</th>\n",
       "      <th>health_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>805.0</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1213.0</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>930.0</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>913.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>788.0</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>989.0</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>856.0</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>729.0</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>863.0</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>725.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4836 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        male  education  distance_store  health_status\n",
       "0      805.0      0.093           0.269              0\n",
       "1     1213.0      0.281           0.601              0\n",
       "2      930.0      0.299           0.310              0\n",
       "3      913.0      0.190           0.396              0\n",
       "4      788.0      0.296           0.222              0\n",
       "...      ...        ...             ...            ...\n",
       "4831   989.0      0.138           0.418              1\n",
       "4832   856.0      0.093           0.299              1\n",
       "4833   729.0      0.351           0.775              1\n",
       "4834   863.0      0.257           0.256              1\n",
       "4835   725.0      0.100           0.066              1\n",
       "\n",
       "[4836 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading the preprocessed dataset \n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'/Users/noortje/Documents/880502-M-18 | Master Thesis/Python/Dataset RQ4.csv')\n",
    "\n",
    "## Reviewing the loaded dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy on validation data = 0.5573513610315186\n",
      "Recall-score (positive class) on validation data = 0.65625\n",
      "----------------------------------------------------------------------------------------------\n",
      "Checking for model overfitting by computing the evaluation metric on training data:\n",
      "-----------------\n",
      "Balanced accuracy on training data = 0.5666938782637546\n",
      "Recall-score (positive class) on training data = 0.6775818639798489\n"
     ]
    }
   ],
   "source": [
    "## RANDOM FOREST: IMPLEMENTATION FOR RQ4\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "## Assign data to X and Y\n",
    "X = df.drop('health_status', axis=1).values\n",
    "Y = df['health_status'].values\n",
    "\n",
    "## Splitting the data into a training and testing set\n",
    "X_main, X_test, y_main, y_test = train_test_split(X, Y, \n",
    "test_size = 0.37, random_state = 101)\n",
    "\n",
    "## Splitting the main set into a training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, \n",
    "test_size = 0.23, random_state = 101)\n",
    "\n",
    "## Feature scaling using RobustScaler to put all features to the same scale\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "## Assigning sample weights\n",
    "weights = np.zeros(len(y_train))\n",
    "weights[y_train == 1] = 0.5\n",
    "weights[y_train == 0] = 0.5\n",
    "\n",
    "## Defining and fitting the model\n",
    "model = RandomForestClassifier(random_state=101, n_estimators = 60,\n",
    "min_samples_split = 70, min_samples_leaf = 40, max_samples = 100,\n",
    "max_features = 'sqrt', max_depth = 10)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, sample_weight = weights)\n",
    "\n",
    "## Making predictions for every partition of the dataset \n",
    "pred_val = model.predict(X_val_scaled)\n",
    "pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "## Evaluating the model performance\n",
    "print('Balanced accuracy on validation data =', balanced_accuracy_score(y_val, pred_val))\n",
    "print('Recall-score (positive class) on validation data =', recall_score(y_val, pred_val, pos_label = 1))\n",
    "print('----------------------------------------------------------------------------------------------')\n",
    "print('Checking for model overfitting by computing the evaluation metric on training data:')\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on training data =', balanced_accuracy_score(y_train, pred_train))\n",
    "print('Recall-score (positive class) on training data =', recall_score(y_train, pred_train, pos_label = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the final model performance by computing the evaluation metric on unseen testing data:\n",
      "-----------------\n",
      "Balanced accuracy on testing data = 0.5305074160811866\n",
      "Recall-score (positive class) on testing data = 0.6457142857142857\n",
      "-----------------\n",
      "Balanced accuracy on training data = 0.5635872346902652\n",
      "Recall-score (positive class) on training data = 0.6714193130265717\n"
     ]
    }
   ],
   "source": [
    "## FINAL TRAINING ITERATION FOR RF - RQ4\n",
    "## Feature scaling using RobustScaler to put all features to the same scale\n",
    "scaler = RobustScaler()\n",
    "X_main_scaled = scaler.fit_transform(X_main)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "## Assigning sample weights\n",
    "weights = np.zeros(len(y_train))\n",
    "weights[y_train == 1] = 0.5\n",
    "weights[y_train == 0] = 0.5\n",
    "\n",
    "## Defining and fitting the model\n",
    "model = RandomForestClassifier(random_state=101, n_estimators = 60,\n",
    "min_samples_split = 70, min_samples_leaf = 40, max_samples = 100,\n",
    "max_features = 'sqrt', max_depth = 10)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, sample_weight = weights)\n",
    "\n",
    "## Making predictions for the testing partition of the dataset\n",
    "pred_test = model.predict(X_test_scaled)\n",
    "pred_train = model.predict(X_main_scaled)\n",
    "\n",
    "## Unbiased estimate on unseen data\n",
    "print('Checking the final model performance by computing the evaluation metric on unseen testing data:')\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on testing data =', balanced_accuracy_score(y_test, pred_test))\n",
    "print('Recall-score (positive class) on testing data =', recall_score(y_test, pred_test, pos_label = 1))\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on training data =', balanced_accuracy_score(y_main, pred_train))\n",
    "print('Recall-score (positive class) on training data =', recall_score(y_main, pred_train, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=200, min_samples_leaf=44, min_samples_split=13, n_estimators=500; total time=   2.5s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=200, min_samples_leaf=44, min_samples_split=13, n_estimators=500; total time=   2.5s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=200, min_samples_leaf=44, min_samples_split=13, n_estimators=500; total time=   2.4s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=200, min_samples_leaf=44, min_samples_split=13, n_estimators=500; total time=   2.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=44, min_samples_split=37, n_estimators=100; total time=   0.6s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=44, min_samples_split=37, n_estimators=100; total time=   0.6s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=44, min_samples_split=37, n_estimators=100; total time=   0.6s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=44, min_samples_split=37, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=44, min_samples_split=37, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=22, min_samples_split=50, n_estimators=300; total time=   1.6s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=200, min_samples_leaf=44, min_samples_split=13, n_estimators=500; total time=   2.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=22, min_samples_split=50, n_estimators=300; total time=   1.7s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=22, min_samples_split=50, n_estimators=300; total time=   1.7s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=700, min_samples_leaf=1, min_samples_split=1, n_estimators=350; total time=   0.4s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=700, min_samples_leaf=1, min_samples_split=1, n_estimators=350; total time=   0.4s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=700, min_samples_leaf=1, min_samples_split=1, n_estimators=350; total time=   0.4s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=700, min_samples_leaf=1, min_samples_split=1, n_estimators=350; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=22, min_samples_split=50, n_estimators=300; total time=   1.8s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=700, min_samples_leaf=1, min_samples_split=1, n_estimators=350; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=600, min_samples_leaf=22, min_samples_split=50, n_estimators=300; total time=   1.7s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=200, min_samples_leaf=17, min_samples_split=50, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=200, min_samples_leaf=17, min_samples_split=50, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=1000, min_samples_leaf=28, min_samples_split=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=200, min_samples_leaf=17, min_samples_split=50, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=1000, min_samples_leaf=28, min_samples_split=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=1000, min_samples_leaf=28, min_samples_split=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=200, min_samples_leaf=17, min_samples_split=50, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=1000, min_samples_leaf=28, min_samples_split=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=15, max_features=log2, max_samples=1000, min_samples_leaf=28, min_samples_split=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=3, max_features=log2, max_samples=200, min_samples_leaf=17, min_samples_split=50, n_estimators=300; total time=   1.5s\n",
      "[CV] END max_depth=6, max_features=sqrt, max_samples=500, min_samples_leaf=39, min_samples_split=13, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=6, max_features=sqrt, max_samples=500, min_samples_leaf=39, min_samples_split=13, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=6, max_features=sqrt, max_samples=500, min_samples_leaf=39, min_samples_split=13, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=6, max_features=sqrt, max_samples=500, min_samples_leaf=39, min_samples_split=13, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=9, max_features=sqrt, max_samples=100, min_samples_leaf=50, min_samples_split=13, n_estimators=200; total time=   1.1s\n",
      "[CV] END max_depth=9, max_features=sqrt, max_samples=100, min_samples_leaf=50, min_samples_split=13, n_estimators=200; total time=   1.0s\n",
      "[CV] END max_depth=6, max_features=sqrt, max_samples=500, min_samples_leaf=39, min_samples_split=13, n_estimators=200; total time=   1.2s\n",
      "[CV] END max_depth=9, max_features=sqrt, max_samples=100, min_samples_leaf=50, min_samples_split=13, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=9, max_features=sqrt, max_samples=100, min_samples_leaf=50, min_samples_split=13, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=9, max_features=sqrt, max_samples=100, min_samples_leaf=50, min_samples_split=13, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=12, max_features=sqrt, max_samples=500, min_samples_leaf=11, min_samples_split=37, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=12, max_features=sqrt, max_samples=500, min_samples_leaf=11, min_samples_split=37, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=12, max_features=sqrt, max_samples=500, min_samples_leaf=11, min_samples_split=37, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=12, max_features=sqrt, max_samples=500, min_samples_leaf=11, min_samples_split=37, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=500, min_samples_leaf=39, min_samples_split=1, n_estimators=400; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=500, min_samples_leaf=39, min_samples_split=1, n_estimators=400; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=500, min_samples_leaf=39, min_samples_split=1, n_estimators=400; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=500, min_samples_leaf=39, min_samples_split=1, n_estimators=400; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=500, min_samples_leaf=39, min_samples_split=1, n_estimators=400; total time=   0.4s\n",
      "[CV] END max_depth=12, max_features=sqrt, max_samples=500, min_samples_leaf=11, min_samples_split=37, n_estimators=400; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/noortje/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.56289979 0.54882729 0.54498934        nan 0.56332623        nan\n",
      " 0.54882729 0.50788913 0.54072495        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 300,\n",
       " 'min_samples_split': 50,\n",
       " 'min_samples_leaf': 17,\n",
       " 'max_samples': 200,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HYPERPARAMETER TUNING PROCESS FOR RQ4 - RANDOM FOREST\n",
    "## Source of code: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 \n",
    "## Random search iteration 1: \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "## Creating the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start=50, stop = 500, num = 10)],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [int(x) for x in np.linspace(start=3, stop = 15, num = 5)],\n",
    "            'min_samples_split': [int(x) for x in np.linspace(start=1, stop = 50, num = 5)],\n",
    "            'min_samples_leaf': [int(x) for x in np.linspace(start=1, stop = 50, num = 10)],\n",
    "            'max_samples': [int(x) for x in np.linspace(start=100, stop = 1000, num = 10)]}\n",
    "\n",
    "## Creating the model to tune\n",
    "rf = RandomForestClassifier(random_state=101)\n",
    "\n",
    "## Random search procedure of parameters, using 3 fold cross validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=101, n_jobs = -1)\n",
    "\n",
    "## Fitting the random search model to the previously defined resampled dataset using SMOTE\n",
    "rf_random.fit(X_train_scaled, y_train, sample_weight = weights)\n",
    "\n",
    "## Retrieving the best model parameters according to this iteration\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=8, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=20, n_estimators=266; total time=   1.8s\n",
      "[CV] END max_depth=8, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=20, n_estimators=266; total time=   1.9s\n",
      "[CV] END max_depth=8, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=20, n_estimators=266; total time=   1.9s\n",
      "[CV] END max_depth=8, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=20, n_estimators=266; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=155, min_samples_leaf=40, min_samples_split=80, n_estimators=266; total time=   2.1s\n",
      "[CV] END max_depth=8, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=20, n_estimators=266; total time=   2.1s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=155, min_samples_leaf=40, min_samples_split=80, n_estimators=266; total time=   2.1s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=155, min_samples_leaf=40, min_samples_split=80, n_estimators=266; total time=   2.0s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=155, min_samples_leaf=40, min_samples_split=80, n_estimators=266; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=155, min_samples_leaf=40, min_samples_split=80, n_estimators=266; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=100, min_samples_leaf=50, min_samples_split=100, n_estimators=533; total time=   3.5s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=100, min_samples_leaf=50, min_samples_split=100, n_estimators=533; total time=   3.5s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=100, min_samples_leaf=50, min_samples_split=100, n_estimators=533; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=100, min_samples_leaf=50, min_samples_split=100, n_estimators=533; total time=   3.3s\n",
      "[CV] END max_depth=10, max_features=log2, max_samples=100, min_samples_leaf=50, min_samples_split=100, n_estimators=533; total time=   3.0s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=211, min_samples_leaf=10, min_samples_split=20, n_estimators=600; total time=   3.6s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=211, min_samples_leaf=10, min_samples_split=20, n_estimators=600; total time=   3.3s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=211, min_samples_leaf=10, min_samples_split=20, n_estimators=600; total time=   3.3s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=211, min_samples_leaf=10, min_samples_split=20, n_estimators=600; total time=   3.2s\n",
      "[CV] END max_depth=6, max_features=log2, max_samples=211, min_samples_leaf=10, min_samples_split=20, n_estimators=600; total time=   3.2s\n",
      "[CV] END max_depth=5, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=100, n_estimators=533; total time=   2.9s\n",
      "[CV] END max_depth=5, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=100, n_estimators=533; total time=   2.9s\n",
      "[CV] END max_depth=5, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=100, n_estimators=533; total time=   2.8s\n",
      "[CV] END max_depth=5, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=100, n_estimators=533; total time=   2.8s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=488, min_samples_leaf=50, min_samples_split=80, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=log2, max_samples=211, min_samples_leaf=40, min_samples_split=100, n_estimators=533; total time=   3.0s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=488, min_samples_leaf=50, min_samples_split=80, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=488, min_samples_leaf=50, min_samples_split=80, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=488, min_samples_leaf=50, min_samples_split=80, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=488, min_samples_leaf=50, min_samples_split=80, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=7, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=40, n_estimators=400; total time=   2.4s\n",
      "[CV] END max_depth=7, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=40, n_estimators=400; total time=   2.4s\n",
      "[CV] END max_depth=7, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=40, n_estimators=400; total time=   2.4s\n",
      "[CV] END max_depth=7, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=40, n_estimators=400; total time=   2.4s\n",
      "[CV] END max_depth=7, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=40, n_estimators=400; total time=   2.5s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=155, min_samples_leaf=50, min_samples_split=40, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=155, min_samples_leaf=50, min_samples_split=40, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=155, min_samples_leaf=50, min_samples_split=40, n_estimators=400; total time=   2.2s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=155, min_samples_leaf=50, min_samples_split=40, n_estimators=400; total time=   2.3s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=155, min_samples_leaf=50, min_samples_split=40, n_estimators=400; total time=   2.1s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=20, n_estimators=666; total time=   4.0s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=20, n_estimators=666; total time=   4.0s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=20, n_estimators=666; total time=   3.9s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=20, n_estimators=666; total time=   3.9s\n",
      "[CV] END max_depth=9, max_features=log2, max_samples=600, min_samples_leaf=30, min_samples_split=20, n_estimators=666; total time=   3.9s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=544, min_samples_leaf=20, min_samples_split=20, n_estimators=600; total time=   3.6s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=544, min_samples_leaf=20, min_samples_split=20, n_estimators=600; total time=   3.5s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=544, min_samples_leaf=20, min_samples_split=20, n_estimators=600; total time=   3.6s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=544, min_samples_leaf=20, min_samples_split=20, n_estimators=600; total time=   2.6s\n",
      "[CV] END max_depth=11, max_features=log2, max_samples=544, min_samples_leaf=20, min_samples_split=20, n_estimators=600; total time=   2.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 266,\n",
       " 'min_samples_split': 20,\n",
       " 'min_samples_leaf': 40,\n",
       " 'max_samples': 211,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 8}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HYPERPARAMETER TUNING PROCESS FOR RQ4 - RANDOM FOREST\n",
    "## Source of code: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 \n",
    "## Random search iteration 2: \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "## Creating the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start=200, stop = 800, num = 10)],\n",
    "            'max_features': ['log2'],\n",
    "            'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "            'min_samples_split': [int(x) for x in np.linspace(start=20, stop = 100, num = 5)],\n",
    "            'min_samples_leaf': [int(x) for x in np.linspace(start=10, stop = 50, num = 5)],\n",
    "            'max_samples': [int(x) for x in np.linspace(start=100, stop = 600, num = 10)]}\n",
    "\n",
    "## Creating the model to tune\n",
    "rf = RandomForestClassifier(random_state=101)\n",
    "\n",
    "## Random search procedure of parameters, using 3 fold cross validation\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=101, n_jobs = -1)\n",
    "\n",
    "## Fitting the random search model to the previously defined resampled dataset using SMOTE\n",
    "rf_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "## Retrieving the best model parameters according to this iteration\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy on validation data = 0.5284742120343839\n",
      "Recall-score (positive class) on validation data = 0.375\n",
      "----------------------------------------------------------------------------------------------\n",
      "Checking for model overfitting by computing the evaluation metric on training data:\n",
      "-----------------\n",
      "Balanced accuracy on training data = 0.504271638676556\n",
      "Recall-score (positive class) on training data = 0.35516372795969775\n"
     ]
    }
   ],
   "source": [
    "## GRADIENT BOOSTED DECISION TREE: IMPLEMENTATION FOR RQ4\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "## Assign data to X and Y\n",
    "X = df.drop('health_status', axis=1).values\n",
    "Y = df['health_status'].values\n",
    "\n",
    "## Splitting the data into a training and testing set\n",
    "X_main, X_test, y_main, y_test = train_test_split(X, Y, \n",
    "test_size = 0.37, random_state = 101)\n",
    "\n",
    "## Splitting the main set into a training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, \n",
    "test_size = 0.23, random_state = 101)\n",
    "\n",
    "## Feature scaling using RobustScaler to put all features to the same scale\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "## Assigning sample weights\n",
    "weights = np.zeros(len(y_train))\n",
    "weights[y_train == 1] = 0.5\n",
    "weights[y_train == 0] = 0.5\n",
    "\n",
    "## Defining and fitting the model\n",
    "model = GradientBoostingClassifier(random_state=101, subsample = 0.1,\n",
    "n_estimators = 50, min_samples_split = 40, min_samples_leaf = 50, max_depth = 3,\n",
    "learning_rate = 0.8)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, sample_weight = weights)\n",
    "\n",
    "## Making predictions for every partition of the dataset \n",
    "pred_val = model.predict(X_val)\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "## Evaluating the model performance\n",
    "print('Balanced accuracy on validation data =', balanced_accuracy_score(y_val, pred_val))\n",
    "print('Recall-score (positive class) on validation data =', recall_score(y_val, pred_val, pos_label = 1))\n",
    "print('----------------------------------------------------------------------------------------------')\n",
    "print('Checking for model overfitting by computing the evaluation metric on training data:')\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on training data =', balanced_accuracy_score(y_train, pred_train))\n",
    "print('Recall-score (positive class) on training data =', recall_score(y_train, pred_train, pos_label = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the final model performance by computing the evaluation metric on unseen testing data:\n",
      "-----------------\n",
      "Balanced accuracy on testing data = 0.5120936768149883\n",
      "Recall-score (positive class) on testing data = 0.5782857142857143\n",
      "-----------------\n",
      "Balanced accuracy on training data = 0.5614519502796093\n",
      "Recall-score (positive class) on training data = 0.6318859364873622\n"
     ]
    }
   ],
   "source": [
    "## FINAL TRAINING ITERATION FOR GBDT - RQ4\n",
    "## Feature scaling using RobustScaler to put all features to the same scale\n",
    "scaler = RobustScaler()\n",
    "X_main_scaled = scaler.fit_transform(X_main)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "## Assigning sample weights\n",
    "weights = np.zeros(len(y_main))\n",
    "weights[y_main == 1] = 0.5\n",
    "weights[y_main == 0] = 0.5\n",
    "\n",
    "## Defining and fitting the model\n",
    "model = GradientBoostingClassifier(random_state=101, subsample = 0.1,\n",
    "n_estimators = 50, min_samples_split = 40, min_samples_leaf = 50, max_depth = 3,\n",
    "learning_rate = 0.8)\n",
    "\n",
    "model.fit(X_main_scaled, y_main, sample_weight = weights)\n",
    "\n",
    "## Making predictions for the testing partition of the dataset\n",
    "pred_test = model.predict(X_test_scaled)\n",
    "pred_train = model.predict(X_main_scaled)\n",
    "\n",
    "## Unbiased estimate on unseen data\n",
    "print('Checking the final model performance by computing the evaluation metric on unseen testing data:')\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on testing data =', balanced_accuracy_score(y_test, pred_test))\n",
    "print('Recall-score (positive class) on testing data =', recall_score(y_test, pred_test, pos_label = 1))\n",
    "print('-----------------')\n",
    "print('Balanced accuracy on training data =', balanced_accuracy_score(y_main, pred_train))\n",
    "print('Recall-score (positive class) on training data =', recall_score(y_main, pred_train, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HYPERPARAMETER TUNING PROCESS FOR RQ4 - GRADIENT BOOSTED DECISION TREE\n",
    "## Source of code: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 \n",
    "## Random search iteration 1: \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "## Creating the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start=50, stop = 700, num = 10)],\n",
    "            'learning_rate': [0.2, 0.5, 0.7, 0.9],\n",
    "            'max_depth': [int(x) for x in np.linspace(start=3, stop = 15, num = 5)],\n",
    "            'min_samples_split': [int(x) for x in np.linspace(start=5, stop = 100, num = 5)],\n",
    "            'min_samples_leaf': [int(x) for x in np.linspace(start=5, stop = 100, num = 10)],\n",
    "            'subsample': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0, 1.3, 1.5]}\n",
    "\n",
    "## Creating the model to tune\n",
    "gbdt = GradientBoostingClassifier(random_state=101)\n",
    "\n",
    "## Random search procedure of parameters, using 3 fold cross validation\n",
    "gbdt_random = RandomizedSearchCV(estimator = gbdt, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=101, n_jobs = -1)\n",
    "\n",
    "## Fitting the random search model to the previously defined resampled dataset using SMOTE\n",
    "gbdt_random.fit(X_train_scaled, y_train, sample_weight = weights)\n",
    "\n",
    "## Retrieving the best model parameters according to this iteration\n",
    "gbdt_random.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95c1dfe8ffb709abbc356f0fa7ea94ea0a57bac032ee04c57783b4e2c1c4dc4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
